@article{eeee2c9e5e7a4c9896f680eeb106c425,
title = "Semantic-aware neural style transfer",
abstract = "This study proposes a semantic-aware style transfer method for resolving semantic mismatch problems in existing algorithms. As the primary focus of this study, the consideration of semantic matching is expected to improve the quality of artistic style transfer. Here, each image is partitioned into several semantic regions for both a target photograph and a source painting. All partitioned regions of the target are then associated with one of the partitioned regions in the source according to their semantic interpretation. Given a pair of target and source regions, style is learned from the source region whereas content is learned from the target region. By integrating both the style and content components, we can successfully generate a stylized output. Unlike previous approaches, we obtain the best semantic match between regions using word embeddings. Thus, we guarantee that semantic matching is always established between the target and source. Moreover, it is unreliable to partition a painting using existing algorithms because of statistical gaps between the real photographs and paintings. To bridge such gaps, we apply a domain adaptation technique on the source painting to extract its semantic regions. We evaluated the effectiveness of the proposed algorithm based on a thorough experimental analysis and comparison. Through a user study, it is confirmed that semantic information considerably influences the quality assessment of style transfer.",
author = "Park, {Joo Hyun} and Song Park and Hyunjung Shim",
note = "Funding Information: This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the MSIP ( NRF-2019R1A2C2006123 ), the MSIT (Ministry of Science and ICT), Korea, under the “ICT Consilience Creative Program” ( IITP-2018-2017-0-01015 ) supervised by the IITP (Institute of Information & Communications Technology Planning & Evaluation), and the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program ( IITP-2019-2016-0-00288 ) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation). Funding Information: This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF)funded by the MSIP (NRF-2019R1A2C2006123), the MSIT (Ministry of Science and ICT), Korea, under the “ICT Consilience Creative Program” (IITP-2018-2017-0-01015)supervised by the IITP (Institute of Information & Communications Technology Planning & Evaluation), and the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center)support program (IITP-2019-2016-0-00288)supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation). Publisher Copyright: {\textcopyright} 2019 Elsevier B.V.",
year = "2019",
month = jul,
doi = "10.1016/j.imavis.2019.04.001",
language = "English",
volume = "87",
pages = "13--23",
journal = "Image and Vision Computing",
issn = "0262-8856",
publisher = "Elsevier Limited",
}
